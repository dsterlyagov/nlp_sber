{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6698d25-0ac3-4c8e-846e-1e39ac4cb02f",
   "metadata": {},
   "source": [
    "Negative Sampling — это метод, который используют в задаче обучения на больших данных, особенно в нейронных сетях, для экономии вычислительных ресурсов и уменьшения сложности задачи. Наиболее популярное его применение — при обучении моделей для обработки естественного языка (NLP), таких как Word2Vec.\n",
    "\n",
    "В Word2Vec задача заключается в обучении модели, которая предсказывает вероятность совместного появления слов. Полное предсказание для всех слов (softmax по всему словарю) будет дорогостоящим, поэтому вместо вычисления вероятности для всех слов, выбираются только несколько \"отрицательных\" примеров.\n",
    "\n",
    "Давайте разберем этот процесс по шагам и реализуем его на Python.\n",
    "\n",
    "Пример с использованием Negative Sampling в Word2Vec\n",
    "Рассмотрим небольшую реализацию Skip-gram модели с Negative Sampling. Мы обучим модель предсказывать вероятность того, что пара слов (слово-контекст) является настоящей или сгенерированной (отрицательной).\n",
    "\n",
    "Шаги:\n",
    "Подготовка данных — создаем данные и словарь.\n",
    "Создание положительных пар — пары из текущего слова и его контекста.\n",
    "Negative Sampling — для каждой пары из положительного примера выбираем несколько отрицательных примеров.\n",
    "Обучение модели — минимизируем ошибку на положительных парах и максимизируем на отрицательных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aec2f52-c292-4273-a919-210edbebf20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'but', 'are', 'an', 'what', 'do', 'repeatedly', 'then', 'excellence', 'is', 'habit', 'act', 'not', 'we']\n",
      "Word to Index Mapping: {'a': 0, 'but': 1, 'are': 2, 'an': 3, 'what': 4, 'do': 5, 'repeatedly': 6, 'then': 7, 'excellence': 8, 'is': 9, 'habit': 10, 'act': 11, 'not': 12, 'we': 13}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Пример текста\n",
    "text = \"we are what we repeatedly do excellence then is not an act but a habit\".split()\n",
    "\n",
    "# Словарь\n",
    "vocab = list(set(text))\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "index_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Word to Index Mapping:\", word_to_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ee47c-a7dd-407b-bfa4-be5491ef66bb",
   "metadata": {},
   "source": [
    "2. Создание положительных пар (пары (слово, контекст))\n",
    "Создадим пары (слово, контекст) для каждого слова в предложении с окном размером window_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7788410-177e-493d-a941-1d5e11dd538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive pairs (word index, context index): [(13, 2), (13, 4), (2, 13), (2, 4), (2, 13), (4, 13), (4, 2), (4, 13), (4, 6), (13, 2), (13, 4), (13, 6), (13, 5), (6, 4), (6, 13), (6, 5), (6, 8), (5, 13), (5, 6), (5, 8), (5, 7), (8, 6), (8, 5), (8, 7), (8, 9), (7, 5), (7, 8), (7, 9), (7, 12), (9, 8), (9, 7), (9, 12), (9, 3), (12, 7), (12, 9), (12, 3), (12, 11), (3, 9), (3, 12), (3, 11), (3, 1), (11, 12), (11, 3), (11, 1), (11, 0), (1, 3), (1, 11), (1, 0), (1, 10), (0, 11), (0, 1), (0, 10), (10, 1), (10, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Параметры\n",
    "window_size = 2\n",
    "\n",
    "# Положительные пары\n",
    "def generate_positive_pairs(text, window_size):\n",
    "    positive_pairs = []\n",
    "    for i, word in enumerate(text):\n",
    "        target_word_idx = word_to_index[word]\n",
    "        context_indices = range(max(0, i - window_size), min(len(text), i + window_size + 1))\n",
    "        for j in context_indices:\n",
    "            if i != j:\n",
    "                context_word_idx = word_to_index[text[j]]\n",
    "                positive_pairs.append((target_word_idx, context_word_idx))\n",
    "    return positive_pairs\n",
    "\n",
    "positive_pairs = generate_positive_pairs(text, window_size)\n",
    "print(\"Positive pairs (word index, context index):\", positive_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e1264-8f94-4f1c-b274-d0139b4dd6d9",
   "metadata": {},
   "source": [
    "3. Negative Sampling\n",
    "Теперь для каждого (слово, контекст) выбираем отрицательные примеры — слова, которые не встречаются в этом контексте. Обычно их выбирают случайно, но чаще встречающиеся слова могут появляться с большей вероятностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0d9171-712a-4a56-b770-a619a7527da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples (word index, negative context indices): [(13, [3, 8, 3]), (13, [11, 11, 11]), (2, [0, 12, 8]), (2, [6, 8, 3]), (2, [1, 7, 8]), (4, [0, 4, 12]), (4, [4, 9, 6]), (4, [4, 9, 0]), (4, [1, 11, 4]), (13, [12, 0, 9]), (13, [2, 5, 1]), (13, [0, 13, 4]), (13, [11, 6, 8]), (6, [1, 3, 3]), (6, [2, 11, 12]), (6, [2, 8, 6]), (6, [10, 13, 4]), (5, [5, 11, 12]), (5, [0, 0, 7]), (5, [6, 3, 1]), (5, [0, 12, 13]), (8, [12, 13, 3]), (8, [8, 3, 1]), (8, [12, 11, 1]), (8, [12, 2, 7]), (7, [2, 12, 13]), (7, [12, 13, 13]), (7, [13, 13, 8]), (7, [0, 5, 11]), (9, [5, 1, 4]), (9, [11, 0, 6]), (9, [1, 2, 0]), (9, [13, 7, 4]), (12, [9, 9, 13]), (12, [12, 10, 7]), (12, [5, 13, 13]), (12, [2, 4, 13]), (3, [8, 11, 6]), (3, [9, 5, 4]), (3, [1, 10, 13]), (3, [13, 4, 6]), (11, [6, 11, 1]), (11, [8, 13, 12]), (11, [6, 13, 9]), (11, [10, 1, 1]), (1, [10, 0, 0]), (1, [12, 0, 10]), (1, [1, 5, 13]), (1, [7, 12, 9]), (0, [1, 6, 7]), (0, [7, 8, 7]), (0, [12, 12, 4]), (10, [6, 10, 3]), (10, [7, 4, 4])]\n"
     ]
    }
   ],
   "source": [
    "# Выборка отрицательных примеров\n",
    "def generate_negative_samples(positive_pairs, num_negative_samples, vocab_size):\n",
    "    negative_samples = []\n",
    "    word_counts = Counter([pair[0] for pair in positive_pairs])\n",
    "    total_count = sum(word_counts.values())\n",
    "    \n",
    "    # Вероятность для каждого слова\n",
    "    sampling_prob = {word: count/total_count for word, count in word_counts.items()}\n",
    "    \n",
    "    for target_word, context_word in positive_pairs:\n",
    "        negative_context_words = []\n",
    "        \n",
    "        while len(negative_context_words) < num_negative_samples:\n",
    "            negative_sample = random.randint(0, vocab_size - 1)\n",
    "            if negative_sample != context_word:\n",
    "                negative_context_words.append(negative_sample)\n",
    "        \n",
    "        negative_samples.append((target_word, negative_context_words))\n",
    "    \n",
    "    return negative_samples\n",
    "\n",
    "num_negative_samples = 3\n",
    "negative_samples = generate_negative_samples(positive_pairs, num_negative_samples, vocab_size)\n",
    "print(\"Negative samples (word index, negative context indices):\", negative_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad69c1f-53cb-4f72-abdc-3b6ebd34c4d1",
   "metadata": {},
   "source": [
    "4. Обучение модели\n",
    "Теперь, когда у нас есть положительные и отрицательные пары, обучим модель на Skip-gram с Negative Sampling. Мы будем использовать простую нейронную сеть с вложениями (embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d665fe1-eeb8-4f14-8e71-1157b8d8b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 9.6720\n",
      "Epoch 1, Loss: 8.8416\n",
      "Epoch 2, Loss: 8.1147\n",
      "Epoch 3, Loss: 7.4441\n",
      "Epoch 4, Loss: 6.8280\n",
      "Epoch 5, Loss: 6.2648\n",
      "Epoch 6, Loss: 5.7515\n",
      "Epoch 7, Loss: 5.2842\n",
      "Epoch 8, Loss: 4.8580\n",
      "Epoch 9, Loss: 4.4680\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Skip-gram модель с отрицательной выборкой\n",
    "class SkipGramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramNegSampling, self).__init__()\n",
    "        self.target_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, target_word, context_word, negative_samples):\n",
    "        # Положительная пара\n",
    "        target = self.target_embedding(target_word)  # (1, embedding_dim)\n",
    "        context = self.context_embedding(context_word)  # (1, embedding_dim)\n",
    "        pos_score = torch.mul(target, context).sum(dim=1)  # Складываем по embedding_dim\n",
    "        pos_loss = torch.log(torch.sigmoid(pos_score))  # Лосс для положительных примеров\n",
    "\n",
    "        # Отрицательные примеры\n",
    "        neg_context = self.context_embedding(negative_samples)  # (num_neg_samples, embedding_dim)\n",
    "        neg_score = torch.matmul(neg_context, target.t())  # Скалярное произведение с транспонированием\n",
    "        neg_loss = torch.log(torch.sigmoid(-neg_score)).sum()  # Лосс для отрицательных примеров\n",
    "        \n",
    "        # Общий лосс\n",
    "        return -(pos_loss + neg_loss).mean()\n",
    "\n",
    "# Параметры\n",
    "vocab_size = 100  # Пример размера словаря\n",
    "embedding_dim = 10\n",
    "model = SkipGramNegSampling(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Пример данных для обучения\n",
    "positive_pairs = [(1, 2), (2, 3), (3, 4)]  # Пример пар \"слово-контекст\"\n",
    "negative_samples = {1: [3, 4], 2: [1, 4], 3: [1, 2]}  # Пример отрицательных образцов\n",
    "\n",
    "# Обучение\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for target_word, context_word in positive_pairs:\n",
    "        # Преобразование в тензоры\n",
    "        target_word = torch.tensor([target_word], dtype=torch.long)\n",
    "        context_word = torch.tensor([context_word], dtype=torch.long)\n",
    "        neg_samples = torch.tensor(negative_samples[target_word.item()], dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(target_word, context_word, neg_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a5532-d231-4a34-b0e7-cac7da240181",
   "metadata": {},
   "source": [
    "Пояснение к коду\n",
    "Положительные пары: Создаем положительные пары (слово, контекст) для каждой пары слов в предложении в пределах window_size.\n",
    "Отрицательные примеры: Для каждого (слово, контекст) создаем несколько отрицательных примеров, которые не являются частью контекста.\n",
    "Skip-gram модель с Negative Sampling:\n",
    "Для каждой положительной пары (target, context) мы вычисляем вероятность их совместного появления.\n",
    "Для каждой пары (target, negative_context) мы максимизируем вероятность их раздельного появления.\n",
    "Оптимизация: Обучаем модель, минимизируя ошибку, для того чтобы вложения (embedding) лучше отображали смысловые связи слов.\n",
    "Результаты\n",
    "Модель обучается представлять слова так, чтобы схожие слова имели близкие векторы. Negative Sampling ускоряет обучение, создавая только небольшое количество отрицательных примеров, что делает метод эффективным для задач NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81722ad8-b9f9-4334-ab18-506f74dfd001",
   "metadata": {},
   "source": [
    "Пояснения\n",
    "Положительные пары:\n",
    "\n",
    "Лосс для положительных пар рассчитывается через скалярное произведение target и context, а затем проходит через sigmoid и логарифм, чтобы получить pos_loss.\n",
    "Отрицательные примеры:\n",
    "\n",
    "Используем отрицательные примеры для данного целевого слова, умножая их эмбеддинги на транспонированный вектор target. Это приводит к правильному скалярному произведению по нужной размерности, и на выходе мы получаем оценку neg_score.\n",
    "После sigmoid и логарифма суммируем neg_loss для всех отрицательных примеров.\n",
    "Общий лосс:\n",
    "\n",
    "Общий лосс рассчитывается как отрицательная сумма положительных и отрицательных лоссов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a72c1-fb29-47d1-b99c-20f68a195d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
